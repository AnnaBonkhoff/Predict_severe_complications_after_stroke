{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import interp\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                           initial_list=[], \n",
    "                           threshold_out = 0.01, \n",
    "                           verbose=True):\n",
    "\n",
    "        included = list(initial_list)\n",
    "        while True:\n",
    "            # backward step\n",
    "            model = sm.Logit(y, sm.add_constant(pd.DataFrame(X[included]))).fit(maxiter=200)\n",
    "            # use all coefs except intercept\n",
    "            pvalues = model.pvalues.iloc[1:]\n",
    "            worst_pval = pvalues.max() # null if pvalues is empty\n",
    "            if worst_pval > threshold_out:\n",
    "              #  print(\"Deleted: \")\n",
    "                changed=True\n",
    "                worst_feature = pvalues.idxmax()\n",
    "                included.remove(worst_feature)\n",
    "                #if verbose:\n",
    "                    #print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "            else:\n",
    "                changed=False\n",
    "            if not changed:\n",
    "                break\n",
    "        return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://towardsdatascience.com/how-to-calibrate-undersampled-model-scores-8f3319c1ea5b\n",
    "\n",
    "def calibration(data, no_patients, no_patients_with_complication, downsampled_no_patients, downsampled_no_patients_with_complications):\n",
    "    \n",
    "    calibrated_data = \\\n",
    "    ((data * (no_patients_with_complication / no_patients) / (downsampled_no_patients_with_complications / downsampled_no_patients)) /\n",
    "    ((\n",
    "        (1 - data) * (1 - no_patients_with_complication / no_patients) / (1 - downsampled_no_patients_with_complications / downsampled_no_patients)\n",
    "     ) +\n",
    "     (\n",
    "        data * (no_patients_with_complication / no_patients) / (downsampled_no_patients_with_complications / downsampled_no_patients)\n",
    "     )))\n",
    "\n",
    "    return calibrated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_post = pd.read_csv(\"qs_2016_2017_prediction_mortality_211213.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_post_2016 = qs_post[qs_post[\"dtjahr_a\"]==2016].copy()\n",
    "qs_post_2016 = qs_post_2016.drop(columns=[\"dtjahr_a\",])\n",
    "\n",
    "qs_post_2017 = qs_post[qs_post[\"dtjahr_a\"]==2017].copy()\n",
    "qs_post_2017 = qs_post_2017.drop(columns=[\"dtjahr_a\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_downsamples = 100\n",
    "\n",
    "all_preds_2016 = []\n",
    "all_y_2016 = []\n",
    "\n",
    "all_preds_2017 = []\n",
    "all_y_2017 = []\n",
    "\n",
    "variables = pd.DataFrame({\"Var_nos\": np.zeros(np.shape(qs_post_2016)[1])})\n",
    "variables.index = qs_post_2016.columns\n",
    "\n",
    "tprs_2016 = []\n",
    "tprs_2017 = []\n",
    "test_auc_2016 = []\n",
    "test_auc_2017 = []\n",
    "\n",
    "fraction_pos_all_2017 = []\n",
    "mean_pred_all_2017 = []\n",
    "fraction_pos_all_2017_cal = []\n",
    "mean_pred_all_2017_cal = []\n",
    "brier_score_all = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "test_auc_2017_cal = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for no in range(no_downsamples):\n",
    "    qs_post_2016_0 = qs_post_2016[qs_post_2016[\"tot\"]==0].copy()\n",
    "    qs_post_2016_1 = qs_post_2016[qs_post_2016[\"tot\"]==1].copy()\n",
    "\n",
    "    qs_post_2016_0_sample = qs_post_2016_0.sample(int(np.sum(qs_post_2016[\"tot\"])), replace=False)\n",
    "    qs_post_2016_1_sample = qs_post_2016_1.sample(int(np.sum(qs_post_2016[\"tot\"])), replace=False)\n",
    "\n",
    "    qs_post_2016_joint = pd.concat([qs_post_2016_0_sample, qs_post_2016_1_sample])\n",
    "\n",
    "    y = qs_post_2016_joint[\"tot\"]\n",
    "    qs_post_2016_joint = qs_post_2016_joint.drop([\"tot\"], axis=1)\n",
    "    \n",
    "\n",
    "    qs_post_2017_joint = qs_post_2017.copy()\n",
    "\n",
    "    y_2017 = np.array(qs_post_2017_joint[\"tot\"])\n",
    "    qs_post_2017_joint = qs_post_2017_joint.drop([\"tot\"], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(qs_post_2016_joint, y, test_size=0.2, random_state=no)\n",
    "        \n",
    "    result = stepwise_selection(X_train, y_train, initial_list=list(X_train.columns))\n",
    "        \n",
    "    for n in result:\n",
    "            variables.loc[n] = variables.loc[n]+1\n",
    "    lr = LogisticRegression(random_state=0, solver='liblinear',\n",
    "                         C=10000).fit(X_train[result], y_train)\n",
    "    preds = lr.predict(X_test[result])\n",
    "    preds_2017 = lr.predict(qs_post_2017_joint[result])\n",
    "    all_preds_2016.append(preds)\n",
    "    all_y_2016.append(y_test)\n",
    "    all_preds_2017.append(preds_2017)\n",
    "    all_y_2017.append(y_2017)\n",
    "        \n",
    "    y_proba_2016 = pd.DataFrame(lr.predict_proba(X_test[result])).loc[:,1]\n",
    "            \n",
    "    fpr_2016, tpr_2016, thresholds_2016 = roc_curve(y_test, y_proba_2016)\n",
    "    tprs_2016.append(np.interp(mean_fpr, fpr_2016, tpr_2016))\n",
    "    test_auc_2016.append(roc_auc_score(y_test, y_proba_2016))\n",
    "        \n",
    "    y_proba_2017 = pd.DataFrame(lr.predict_proba(qs_post_2017_joint[result])).loc[:,1]\n",
    "            \n",
    "    fpr_2017, tpr_2017, thresholds_2017 = roc_curve(y_2017, y_proba_2017)\n",
    "    tprs_2017.append(np.interp(mean_fpr, fpr_2017, tpr_2017))\n",
    "    test_auc_2017.append(roc_auc_score(y_2017, y_proba_2017))\n",
    "        \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_2017, y_proba_2017, n_bins=100,strategy='quantile')\n",
    "    brier_score = brier_score_loss(y_2017, y_proba_2017, pos_label=y_2017.max())\n",
    "    brier_score_all.append(brier_score)\n",
    "    fraction_pos_all_2017.append(fraction_of_positives)\n",
    "    mean_pred_all_2017.append(mean_predicted_value)\n",
    "        \n",
    "    calibrated_y_proba_2017 = calibration(y_proba_2017, len(y_2017), np.sum(y_2017), len(y), np.sum(y))\n",
    "    fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(y_2017, calibrated_y_proba_2017, n_bins=100,strategy='quantile')\n",
    "    fraction_pos_all_2017_cal.append(fraction_of_positives_cal)\n",
    "    mean_pred_all_2017_cal.append(mean_predicted_value_cal)\n",
    "    test_auc_2017_cal.append(roc_auc_score(y_2017, calibrated_y_proba_2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(test_auc_2016)\n",
    "scores.columns = [\"validation_auc_2016\"]\n",
    "scores[\"auc_2017\"] = test_auc_2017\n",
    "scores[\"auc_cal_2017\"] = test_auc_2017\n",
    "scores[\"brier_2017\"] = brier_score_all\n",
    "scores.to_csv(\"predictions/Logistic_mortality_noICU_1111.csv\")\n",
    "\n",
    "tprs_long = pd.DataFrame(tprs_2017)\n",
    "tprs_long.to_csv(\"predictions/Logistic_mortality_tprs_noICU_1111.csv\")\n",
    "\n",
    "fraction_pos_all_2017 = pd.DataFrame(fraction_pos_all_2017_cal)\n",
    "fraction_pos_all_2017.to_csv(\"predictions/Logistic_mortality_fraction_pos_cal_noICU_1111.csv\")\n",
    "\n",
    "mean_pred_all_2017 = pd.DataFrame(mean_pred_all_2017_cal)\n",
    "mean_pred_all_2017.to_csv(\"predictions/Logistic_mortality_mean_pred_cal_noICU_1111.csv\")\n",
    "\n",
    "pd.DataFrame(all_preds_2016).to_csv(\"predictions/LR/Logistic_mortality_all_pred_2016_noICU_1111.csv\")\n",
    "pd.DataFrame(all_preds_2017).to_csv(\"predictions/LR/Logistic_mortality_all_pred_2017_noICU_1111.csv\")\n",
    "pd.DataFrame(all_y_2016).to_csv(\"predictions/LR/Logistic_mortality_all_y_2016_noICU_1111.csv\")\n",
    "pd.DataFrame(all_y_2017).to_csv(\"predictions/LR/Logistic_mortality_all_y_2017_noICU_1111.csv\")\n",
    "\n",
    "variables.to_csv(\"predictions/Logistic_mortality_variables_noICU_1111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
