{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(data, train_pop, target_pop, sampled_train_pop, sampled_target_pop):\n",
    "\n",
    "    calibrated_data = \\\n",
    "    ((data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)) /\n",
    "    ((\n",
    "        (1 - data) * (1 - target_pop / train_pop) / (1 - sampled_target_pop / sampled_train_pop)\n",
    "     ) +\n",
    "     (\n",
    "        data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)\n",
    "     )))\n",
    "\n",
    "    return calibrated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_post = pd.read_csv(\"qs_2016_2017_prediction_mortality_211213.csv\", index_col=0)\n",
    "\n",
    "qs_post_2016 = qs_post[qs_post[\"dtjahr_a\"]==2016].copy()\n",
    "qs_post_2016 = qs_post_2016.drop(columns=[\"dtjahr_a\",])\n",
    "qs_post_2017 = qs_post[qs_post[\"dtjahr_a\"]==2017].copy()\n",
    "qs_post_2017 = qs_post_2017.drop(columns=[\"dtjahr_a\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_downsamples = 100\n",
    "inner_cv = 5\n",
    "\n",
    "feature_importances = []\n",
    "\n",
    "\n",
    "all_preds_2016 = []\n",
    "all_y_2016 = []\n",
    "all_preds_2017 = []\n",
    "all_y_2017 = []\n",
    "\n",
    "tprs_2016 = []\n",
    "tprs_2017 = []\n",
    "test_auc_2016 = []\n",
    "test_auc_2017 = []\n",
    "\n",
    "\n",
    "fraction_pos_all_2017 = []\n",
    "mean_pred_all_2017 = []\n",
    "brier_score_all = []\n",
    "\n",
    "fraction_pos_all_2017_cal = []\n",
    "mean_pred_all_2017_cal = []\n",
    "test_auc_2017_cal = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for no in range(no_downsamples):\n",
    "    print(no)\n",
    "    \n",
    "    qs_post_2016_0 = qs_post_2016[qs_post_2016[\"tot\"]==0].copy()\n",
    "    qs_post_2016_1 = qs_post_2016[qs_post_2016[\"tot\"]==1].copy()\n",
    "\n",
    "    qs_post_2016_0_sample = qs_post_2016_0.sample(int(np.sum(qs_post_2016[\"tot\"])), replace=False)\n",
    "    qs_post_2016_1_sample = qs_post_2016_1.sample(int(np.sum(qs_post_2016[\"tot\"])), replace=False)\n",
    "\n",
    "    qs_post_2016_joint = pd.concat([qs_post_2016_0_sample, qs_post_2016_1_sample])\n",
    "\n",
    "    y = np.array(qs_post_2016_joint[\"tot\"])\n",
    "    qs_post_2016_joint = qs_post_2016_joint.drop([\"tot\"], axis=1)\n",
    "    \n",
    "    qs_post_2017_joint = qs_post_2017.copy()\n",
    "\n",
    "    y_2017 = np.array(qs_post_2017_joint[\"tot\"])\n",
    "    qs_post_2017_joint = qs_post_2017_joint.drop([\"tot\"], axis=1)\n",
    "                \n",
    "    pipe = Pipeline([\n",
    "               ('scale', StandardScaler()),\n",
    "                    ('classify', GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                       max_features='log2'))         \n",
    "                                    ])\n",
    "            \n",
    "    N_ESTIMATORS = [100, 300, 500]\n",
    "    MAX_DEPTH = [1,3, 5]\n",
    "    LOSS = ['deviance', 'exponential']\n",
    "    \n",
    "                \n",
    "    parameters = {'classify__n_estimators': N_ESTIMATORS, \n",
    "                  'classify__max_depth': MAX_DEPTH,\n",
    "                  'classify__loss': LOSS}\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(qs_post_2016_joint, y, test_size=0.2, random_state=no)\n",
    "\n",
    "    clf = GridSearchCV(pipe,param_grid=parameters, cv=inner_cv)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    preds = clf.predict(X_test)\n",
    "    preds_2017 = clf.predict(qs_post_2017_joint)\n",
    "    all_preds_2016.append(preds)\n",
    "    all_y_2016.append(y_test)\n",
    "    all_preds_2017.append(preds_2017)\n",
    "    all_y_2017.append(np.array(y_2017).ravel())\n",
    "        \n",
    "    y_proba_2016 = pd.DataFrame(clf.predict_proba(X_test)).loc[:,1]\n",
    "            \n",
    "    fpr_2016, tpr_2016, thresholds_2016 = roc_curve(y_test, y_proba_2016)\n",
    "    tprs_2016.append(np.interp(mean_fpr, fpr_2016, tpr_2016))\n",
    "    test_auc_2016.append(roc_auc_score(y_test, y_proba_2016))\n",
    "        \n",
    "    y_proba_2017 = pd.DataFrame(clf.predict_proba(qs_post_2017_joint)).loc[:,1]\n",
    "            \n",
    "    fpr_2017, tpr_2017, thresholds_2017 = roc_curve(y_2017, y_proba_2017)\n",
    "    tprs_2017.append(np.interp(mean_fpr, fpr_2017, tpr_2017))\n",
    "    test_auc_2017.append(roc_auc_score(y_2017, y_proba_2017))\n",
    "        \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_2017, y_proba_2017, n_bins=10)\n",
    "    brier_score = brier_score_loss(y_2017, y_proba_2017, pos_label=y_2017.max())\n",
    "    brier_score_all.append(brier_score)\n",
    "    fraction_pos_all_2017.append(fraction_of_positives)\n",
    "    mean_pred_all_2017.append(mean_predicted_value)\n",
    "        \n",
    "    calibrated_y_proba_2017 = calibration(y_proba_2017, len(y_2017), np.sum(y_2017), len(y), np.sum(y))\n",
    "    fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(y_2017, calibrated_y_proba_2017, n_bins=100,strategy='quantile')\n",
    "    fraction_pos_all_2017_cal.append(fraction_of_positives_cal)\n",
    "    mean_pred_all_2017_cal.append(mean_predicted_value_cal)\n",
    "    test_auc_2017_cal.append(roc_auc_score(y_2017, calibrated_y_proba_2017))\n",
    "    \n",
    "\n",
    "    feature_importances = np.append(feature_importances, clf.best_estimator_.named_steps[\"classify\"].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving prediction scores\n",
    "\n",
    "scores = pd.DataFrame(test_auc_2016)\n",
    "scores.columns = [\"validation_auc_2016\"]\n",
    "scores[\"auc_2017\"] = test_auc_2017\n",
    "scores[\"auc_cal_2017\"] = test_auc_2017_cal\n",
    "scores[\"brier_2017\"] = brier_score_all\n",
    "\n",
    "scores.to_csv(\"predictions/GBC_mortality_noICU_1111.csv\")\n",
    "\n",
    "tprs_long = pd.DataFrame(tprs_2017)\n",
    "tprs_long.to_csv(\"predictions/GBC_mortality_tprs_noICU_1111.csv\")\n",
    "\n",
    "fraction_pos_all_2017 = pd.DataFrame(fraction_pos_all_2017_cal)\n",
    "fraction_pos_all_2017.to_csv(\"predictions/GBC_mortality_fraction_pos_cal_noICU_1111.csv\")\n",
    "\n",
    "mean_pred_all_2017 = pd.DataFrame(mean_pred_all_2017_cal)\n",
    "mean_pred_all_2017.to_csv(\"predictions/GBC_mortality_mean_pred_cal_noICU_1111.csv\")\n",
    "\n",
    "pd.DataFrame(all_preds_2016).to_csv(\"predictions/GBC/GBC_mortality_all_pred_2016_noICU_1111.csv\")\n",
    "pd.DataFrame(all_preds_2017).to_csv(\"predictions/GBC/GBC_mortality_all_pred_2017_noICU_1111.csv\")\n",
    "pd.DataFrame(all_y_2016).to_csv(\"predictions/GBC/GBC_mortality_all_y_2016_noICU_1111.csv\")\n",
    "pd.DataFrame(all_y_2017).to_csv(\"predictions/GBC/GBC_mortality_all_y_2017_noICU_1111.csv\")\n",
    "\n",
    "overall_feature_importances = pd.DataFrame(np.reshape(feature_importances, (100, -1))).mean(axis=0)\n",
    "overall_feature_importances.index = qs_post_2016_joint.columns\n",
    "overall_feature_importances = overall_feature_importances.sort_values()\n",
    "overall_feature_importances.to_csv(\"predictions/GBC_mortality_fi_noICU_1111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
